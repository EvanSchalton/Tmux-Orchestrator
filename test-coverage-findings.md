# Test Coverage Findings - Tmux Orchestrator Code Review
**Principal Engineer Analysis | 2025-08-15**

## Executive Summary

Test coverage analysis reveals a **critical testing crisis** with only 7% overall coverage (813/11,419 statements). Core system components have 0% coverage, creating significant risks for production deployment and future refactoring efforts.

## Coverage Analysis

### Overall Coverage Statistics
```
Statements:  11,419
Missed:      10,606
Coverage:        7%
```

### Critical Coverage Gaps by Module

#### ðŸ”´ Zero Coverage (Production Risk)
- **daemon_supervisor.py**: 0% (303 lines) - Process management
- **monitor.py**: 0% (2,236 lines) - Core monitoring system
- **monitor_async.py**: 0% (299 lines) - Async monitoring
- **monitor_enhanced.py**: 0% (566 lines) - Enhanced monitoring features
- **recovery_daemon.py**: 0% (234 lines) - Agent recovery logic

#### ðŸŸ¡ Low Coverage (<30%)
- **CLI modules**: ~15% average coverage
- **Server routes**: ~25% average coverage
- **Utils modules**: ~20% average coverage
- **Core business logic**: ~10% average coverage

#### ðŸŸ¢ Adequate Coverage (>60%)
- **Configuration management**: ~65%
- **Basic data models**: ~70%
- **Some utility functions**: ~60%

### Test Distribution Analysis

**Existing Test Files**: 87 test files
**Source Files**: 100+ source files
**Test-to-Source Ratio**: 0.87:1 (Appears adequate but coverage is misleading)

**Test Categories**:
- Unit tests: 45 files (mostly fixtures and basic utilities)
- Integration tests: 12 files (limited scope)
- System tests: 8 files (incomplete workflows)
- Performance tests: 0 files (major gap)

## Critical Testing Gaps

### 1. Core Monitoring System (0% Coverage)
**Impact**: System reliability unknown, refactoring impossible

**Missing Test Coverage**:
```python
# monitor.py - 2,236 lines, 0% coverage
class IdleMonitor:
    def _monitor_cycle(self):          # 138 lines - No tests
    def _check_agent_status(self):     # 139 lines - No tests
    def _check_missing_agents(self):   # 152 lines - No tests
    def _run_monitoring_daemon(self):  # 218 lines - No tests
```

**Recommended Test Structure**:
```python
# tests/core/test_idle_monitor.py
import pytest
from unittest.mock import Mock, patch
from tmux_orchestrator.core.monitor import IdleMonitor

class TestIdleMonitor:
    def test_monitor_cycle_basic_flow(self):
        """Test basic monitoring cycle executes without errors."""
        monitor = IdleMonitor()
        with patch('tmux_orchestrator.utils.tmux.TMUXManager') as mock_tmux:
            mock_tmux.list_agents.return_value = ['test:1', 'test:2']
            # Test should not raise exceptions
            monitor._monitor_cycle()

    def test_agent_status_detection(self):
        """Test agent status detection logic."""
        monitor = IdleMonitor()
        # Mock terminal content for different states
        test_cases = [
            ("normal_prompt", "user@host:~$ ", False),  # Not idle
            ("claude_prompt", "â”‚ > ", True),           # Idle
            ("error_state", "Error:", True),           # Error detected
        ]
        for name, content, expected_idle in test_cases:
            assert monitor._is_idle(content) == expected_idle

    @pytest.mark.asyncio
    async def test_concurrent_agent_monitoring(self):
        """Test monitoring multiple agents concurrently."""
        monitor = IdleMonitor()
        agents = [f"session:window{i}" for i in range(10)]
        # Should complete in <5 seconds for 10 agents
        import time
        start = time.time()
        results = await monitor.monitor_agents_async(agents)
        duration = time.time() - start
        assert duration < 5.0
        assert len(results) == 10
```

### 2. Agent Spawning and Recovery (0% Coverage)
**Impact**: Agent lifecycle management unreliable

**Missing Coverage**:
```python
# spawn_agent.py - Critical business logic, minimal tests
async def spawn_agent(tmux: TMUXManager, request: SpawnAgentRequest):
    # 140 lines of logic, no comprehensive tests

# recovery_daemon.py - 0% coverage
class RecoveryDaemon:
    def recover_failed_agent(self):  # No tests
    def detect_agent_failure(self):  # No tests
```

**Recommended Tests**:
```python
# tests/core/test_spawn_agent.py
import pytest
from tmux_orchestrator.server.tools.spawn_agent import spawn_agent, SpawnAgentRequest

class TestSpawnAgent:
    @pytest.mark.asyncio
    async def test_spawn_agent_success(self):
        """Test successful agent spawning."""
        request = SpawnAgentRequest(
            session_name="test-session",
            agent_type="developer",
            project_path="/test/path"
        )

        with patch('tmux_orchestrator.utils.tmux.TMUXManager') as mock_tmux:
            mock_tmux.has_session.return_value = False
            mock_tmux.create_session.return_value = True
            mock_tmux.send_keys.return_value = True

            result = await spawn_agent(mock_tmux, request)

            assert result.success == True
            assert result.session == "test-session"
            assert result.target == "test-session:Claude-developer"

    @pytest.mark.asyncio
    async def test_spawn_agent_invalid_type(self):
        """Test agent spawning with invalid agent type."""
        request = SpawnAgentRequest(
            session_name="test-session",
            agent_type="invalid-type",
            project_path="/test/path"
        )

        result = await spawn_agent(Mock(), request)

        assert result.success == False
        assert "Invalid agent type" in result.error_message

    @pytest.mark.asyncio
    async def test_spawn_agent_tmux_failure(self):
        """Test agent spawning when tmux operations fail."""
        request = SpawnAgentRequest(
            session_name="test-session",
            agent_type="developer",
            project_path="/test/path"
        )

        with patch('tmux_orchestrator.utils.tmux.TMUXManager') as mock_tmux:
            mock_tmux.has_session.return_value = False
            mock_tmux.create_session.return_value = False  # Simulate failure

            result = await spawn_agent(mock_tmux, request)

            assert result.success == False
            assert "Failed to create new session" in result.error_message
```

### 3. CLI Command Testing (15% Coverage)
**Impact**: User interface reliability unknown

**Current State**: Basic smoke tests only, no comprehensive validation

**Missing Coverage**:
```python
# CLI commands need comprehensive testing
@click.command()
def spawn(ctx, name, target, briefing, working_dir, json):
    # Complex business logic, minimal tests

@click.command()
def monitor(ctx, config_path, status, json):
    # Daemon management logic, no integration tests
```

**Recommended Test Framework**:
```python
# tests/cli/test_cli_commands.py
import pytest
from click.testing import CliRunner
from tmux_orchestrator.cli import cli

class TestCLICommands:
    def setup_method(self):
        self.runner = CliRunner()

    def test_spawn_agent_command(self):
        """Test agent spawning via CLI."""
        result = self.runner.invoke(cli, [
            'spawn', 'agent', 'test-agent', 'test-session:1',
            '--briefing', 'Test briefing',
            '--json'
        ])

        assert result.exit_code == 0
        response = json.loads(result.output)
        assert response['success'] == True

    def test_list_agents_command(self):
        """Test agent listing via CLI."""
        result = self.runner.invoke(cli, ['list', '--json'])

        assert result.exit_code == 0
        response = json.loads(result.output)
        assert 'agents' in response

    @pytest.mark.integration
    def test_monitor_daemon_lifecycle(self):
        """Test monitor daemon start/stop/status."""
        # Start daemon
        result = self.runner.invoke(cli, ['monitor', '--start'])
        assert result.exit_code == 0

        # Check status
        result = self.runner.invoke(cli, ['monitor', '--status', '--json'])
        assert result.exit_code == 0
        status = json.loads(result.output)
        assert status['running'] == True

        # Stop daemon
        result = self.runner.invoke(cli, ['monitor', '--stop'])
        assert result.exit_code == 0
```

### 4. Server Endpoint Testing (25% Coverage)
**Impact**: API reliability and error handling unknown

**Missing Coverage**:
- Error handling paths: <10% coverage
- Input validation: ~30% coverage
- Business logic: ~15% coverage
- Performance characteristics: 0% coverage

**Recommended Test Structure**:
```python
# tests/server/test_agent_routes.py
import pytest
from fastapi.testclient import TestClient
from tmux_orchestrator.server import app

class TestAgentRoutes:
    def setup_method(self):
        self.client = TestClient(app)

    def test_spawn_agent_success(self):
        """Test successful agent spawning via API."""
        response = self.client.post("/spawn_agent", json={
            "session_name": "test-session",
            "agent_type": "developer",
            "project_path": "/test/path"
        })

        assert response.status_code == 200
        data = response.json()
        assert data["success"] == True

    def test_spawn_agent_validation_error(self):
        """Test API validation for invalid requests."""
        response = self.client.post("/spawn_agent", json={
            "session_name": "",  # Invalid empty session
            "agent_type": "invalid",
            "project_path": "/test/path"
        })

        assert response.status_code == 400
        assert "validation error" in response.json()["detail"].lower()

    @pytest.mark.performance
    def test_list_agents_performance(self):
        """Test agent listing performance under load."""
        import time

        # Should complete in <2 seconds even with many agents
        start = time.time()
        response = self.client.get("/list_agents")
        duration = time.time() - start

        assert response.status_code == 200
        assert duration < 2.0
```

## Test Implementation Roadmap

### Phase 1: Critical Coverage (Week 1-2)
**Target**: 30% overall coverage

**Priority 1 - Core System Safety**:
- [ ] Basic daemon_supervisor tests (process lifecycle)
- [ ] Core monitor.py functionality tests (agent detection)
- [ ] Agent spawning success/failure paths
- [ ] Recovery mechanism basic tests

**Implementation**:
```bash
# Create test structure
mkdir -p tests/{core,cli,server,utils,integration}

# Focus on highest-risk modules first
pytest tests/core/test_daemon_supervisor.py -v --cov
pytest tests/core/test_monitor_basic.py -v --cov
pytest tests/core/test_spawn_agent.py -v --cov
```

### Phase 2: Feature Coverage (Week 3-4)
**Target**: 60% overall coverage

**Priority 2 - Feature Validation**:
- [ ] CLI command comprehensive testing
- [ ] Server endpoint error handling
- [ ] Utils module edge cases
- [ ] Integration test scenarios

### Phase 3: Production Readiness (Week 5-6)
**Target**: 80% overall coverage

**Priority 3 - Production Quality**:
- [ ] Performance testing framework
- [ ] Error recovery scenarios
- [ ] Stress testing under load
- [ ] End-to-end workflow validation

## Test Infrastructure Setup

### Testing Dependencies:
```toml
# pyproject.toml additions
[tool.poetry.group.test.dependencies]
pytest = "^7.4.0"
pytest-cov = "^4.1.0"
pytest-asyncio = "^0.21.0"
pytest-mock = "^3.11.0"
pytest-benchmark = "^4.0.0"
httpx = "^0.24.0"  # For FastAPI testing
factory-boy = "^3.3.0"  # For test data generation
```

### Coverage Configuration:
```ini
# .coveragerc
[run]
source = tmux_orchestrator
omit =
    */tests/*
    */venv/*
    */__pycache__/*
    */migrations/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError

[html]
directory = htmlcov
```

### CI/CD Integration:
```yaml
# .github/workflows/test.yml
name: Test Coverage
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          poetry install --with test

      - name: Run tests with coverage
        run: |
          poetry run pytest --cov=tmux_orchestrator --cov-report=xml --cov-report=html

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3

      - name: Fail if coverage below threshold
        run: |
          poetry run coverage report --fail-under=70
```

## Test Quality Standards

### Unit Test Requirements:
- **Coverage**: Minimum 80% line coverage per module
- **Assertions**: Clear, specific assertions with meaningful error messages
- **Isolation**: No dependencies on external systems (tmux mocked)
- **Performance**: Individual tests complete in <100ms

### Integration Test Requirements:
- **Scenarios**: Cover critical user workflows end-to-end
- **Error Handling**: Test failure and recovery scenarios
- **Performance**: Validate performance characteristics under load
- **Resource Cleanup**: Ensure proper cleanup of tmux sessions/processes

### Test Data Management:
```python
# tests/factories.py
import factory
from tmux_orchestrator.server.tools.spawn_agent import SpawnAgentRequest

class SpawnAgentRequestFactory(factory.Factory):
    class Meta:
        model = SpawnAgentRequest

    session_name = factory.Sequence(lambda n: f"test-session-{n}")
    agent_type = factory.Iterator(["developer", "pm", "qa", "devops"])
    project_path = "/tmp/test-project"

# Usage in tests:
def test_multiple_agent_spawn():
    requests = SpawnAgentRequestFactory.build_batch(10)
    # Test spawning 10 different agents
```

## Conclusion

The current 7% test coverage represents a **critical production risk**. The lack of tests for core monitoring and recovery systems means:

1. **Refactoring Risk**: Cannot safely modify monitor.py (2,236 lines)
2. **Regression Risk**: No detection of functionality breaks
3. **Performance Risk**: No validation of scalability claims
4. **Reliability Risk**: Unknown behavior under error conditions

**Immediate Action Required**: Achieve minimum 30% coverage within 2 weeks focusing on core system components.

**Success Metrics**:
- Week 1: daemon_supervisor.py reaches 60% coverage
- Week 2: Basic monitor.py functionality reaches 40% coverage
- Week 3: Agent spawning reaches 80% coverage
- Week 4: CLI commands reach 70% coverage

This testing foundation will enable safe refactoring of the monitor.py God class and provide confidence for production deployment.
