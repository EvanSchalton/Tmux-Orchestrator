"""
Test suite for validating all 6 auto-generated MCP tools functionality.

Comprehensive validation of the fresh MCP server implementation and
all auto-generated tools: list, status, quick-deploy, spawn-orc, execute, reflect.
"""

import json
import subprocess
import time

import pytest


class TestFreshMCPServerValidation:
    """Test the fresh MCP server implementation (mcp_server.py)."""

    def test_fresh_mcp_server_exists_and_executable(self, test_uuid: str) -> None:
        """Test that the fresh MCP server exists and is executable."""
        # Test that the server can be run as a module
        result = subprocess.run(
            ["python", "-c", "import tmux_orchestrator.mcp_server; print('Server module importable')"],
            capture_output=True,
            text=True,
            timeout=5,
        )

        assert result.returncode == 0, f"Fresh MCP server module not importable - Test ID: {test_uuid}"
        assert "Server module importable" in result.stdout, f"Import test failed - Test ID: {test_uuid}"

    def test_fresh_mcp_server_generates_all_6_tools(self, test_uuid: str) -> None:
        """Test that fresh MCP server generates all 6 expected tools."""
        result = subprocess.run(
            ["timeout", "10", "python", "-m", "tmux_orchestrator.mcp_server"], capture_output=True, text=True
        )

        # Check that all 6 tools are generated
        expected_tools = [
            "execute → tmux-orc execute",
            "list → tmux-orc list",
            "quick_deploy → tmux-orc quick-deploy",
            "reflect → tmux-orc reflect",
            "spawn_orc → tmux-orc spawn-orc",
            "status → tmux-orc status",
        ]

        for tool in expected_tools:
            assert tool in result.stderr, f"Tool '{tool}' not generated by fresh server - Test ID: {test_uuid}"

        # Verify server starts successfully
        assert "Starting FastMCP server" in result.stderr, f"Fresh MCP server failed to start - Test ID: {test_uuid}"
        assert (
            "Initializing fresh CLI reflection MCP server" in result.stderr
        ), f"Fresh server not initializing properly - Test ID: {test_uuid}"

    def test_fresh_mcp_server_cli_discovery(self, test_uuid: str) -> None:
        """Test that fresh MCP server discovers CLI structure correctly."""
        result = subprocess.run(
            ["timeout", "10", "python", "-m", "tmux_orchestrator.mcp_server"], capture_output=True, text=True
        )

        # Should discover CLI structure
        assert (
            "Discovering CLI structure via tmux-orc reflect" in result.stderr
        ), f"CLI discovery not initiated - Test ID: {test_uuid}"
        assert (
            "Discovered" in result.stderr and "CLI commands" in result.stderr
        ), f"CLI discovery failed - Test ID: {test_uuid}"

        # Should generate tools from discovery
        assert "Generating MCP tools for" in result.stderr, f"Tool generation not initiated - Test ID: {test_uuid}"


class TestAutoGeneratedToolValidation:
    """Test validation of all 6 auto-generated MCP tools."""

    def test_list_tool_functionality(self, test_uuid: str) -> None:
        """Test that list tool underlying CLI command works correctly."""
        result = subprocess.run(["tmux-orc", "list", "--json"], capture_output=True, text=True, timeout=10)

        assert result.returncode == 0, f"List tool CLI command failed - Test ID: {test_uuid}"

        # Validate JSON output structure
        try:
            data = json.loads(result.stdout)
            assert isinstance(data, list), f"List tool should return array - Test ID: {test_uuid}"

            # If agents exist, validate structure
            for agent in data:
                assert isinstance(agent, dict), f"Agent should be dict - Test ID: {test_uuid}"
                required_fields = ["session", "window", "type", "status", "target"]
                for field in required_fields:
                    assert field in agent, f"List tool missing field '{field}' - Test ID: {test_uuid}"

        except json.JSONDecodeError:
            pytest.fail(f"List tool returned invalid JSON - Test ID: {test_uuid}")

    def test_status_tool_functionality(self, test_uuid: str) -> None:
        """Test that status tool underlying CLI command works correctly."""
        result = subprocess.run(["tmux-orc", "status", "--json"], capture_output=True, text=True, timeout=10)

        assert result.returncode == 0, f"Status tool CLI command failed - Test ID: {test_uuid}"

        # Validate JSON output structure
        try:
            data = json.loads(result.stdout)
            assert isinstance(data, dict), f"Status tool should return object - Test ID: {test_uuid}"

            # Check required top-level fields
            required_fields = ["sessions", "agents", "summary"]
            for field in required_fields:
                assert field in data, f"Status tool missing field '{field}' - Test ID: {test_uuid}"

            # Validate structure
            assert isinstance(data["sessions"], list), f"Status sessions should be array - Test ID: {test_uuid}"
            assert isinstance(data["agents"], list), f"Status agents should be array - Test ID: {test_uuid}"
            assert isinstance(data["summary"], dict), f"Status summary should be object - Test ID: {test_uuid}"

        except json.JSONDecodeError:
            pytest.fail(f"Status tool returned invalid JSON - Test ID: {test_uuid}")

    def test_reflect_tool_functionality(self, test_uuid: str) -> None:
        """Test that reflect tool underlying CLI command works correctly."""
        result = subprocess.run(["tmux-orc", "reflect", "--format", "json"], capture_output=True, text=True, timeout=10)

        assert result.returncode == 0, f"Reflect tool CLI command failed - Test ID: {test_uuid}"

        # Validate CLI structure output
        try:
            data = json.loads(result.stdout)
            assert isinstance(data, dict), f"Reflect tool should return object - Test ID: {test_uuid}"
            assert len(data) > 20, f"Reflect tool should discover many commands - Test ID: {test_uuid}"

            # Verify the 6 core tools are discoverable
            core_tools = ["list", "status", "quick-deploy", "execute", "spawn-orc", "reflect"]
            for tool in core_tools:
                # Convert hyphen to underscore for some commands
                tool_key = tool.replace("-", "_") if "-" in tool else tool
                alt_key = tool

                assert (
                    tool_key in data or alt_key in data
                ), f"Reflect tool should discover '{tool}' command - Test ID: {test_uuid}"

        except json.JSONDecodeError:
            pytest.fail(f"Reflect tool returned invalid JSON - Test ID: {test_uuid}")

    def test_quick_deploy_tool_functionality(self, test_uuid: str) -> None:
        """Test that quick-deploy tool underlying CLI command works correctly."""
        # Test help mode (safer than actual deployment)
        result = subprocess.run(["tmux-orc", "quick-deploy", "--help"], capture_output=True, text=True, timeout=10)

        assert result.returncode == 0, f"Quick-deploy tool help failed - Test ID: {test_uuid}"
        assert "Usage:" in result.stdout, f"Quick-deploy tool help missing usage - Test ID: {test_uuid}"
        assert "quick-deploy" in result.stdout, f"Quick-deploy help missing command name - Test ID: {test_uuid}"

        # Test with JSON flag and invalid arguments (should fail gracefully)
        result = subprocess.run(
            ["tmux-orc", "quick-deploy", "invalid-type", "--json"], capture_output=True, text=True, timeout=10
        )

        # Should fail but not crash
        assert result.returncode != 0, f"Quick-deploy with invalid args should fail - Test ID: {test_uuid}"

    def test_spawn_orc_tool_functionality(self, test_uuid: str) -> None:
        """Test that spawn-orc tool underlying CLI command works correctly."""
        # Test help mode (safer than actual spawning)
        result = subprocess.run(["tmux-orc", "spawn-orc", "--help"], capture_output=True, text=True, timeout=10)

        assert result.returncode == 0, f"Spawn-orc tool help failed - Test ID: {test_uuid}"
        assert "Usage:" in result.stdout, f"Spawn-orc tool help missing usage - Test ID: {test_uuid}"
        assert "spawn-orc" in result.stdout, f"Spawn-orc help missing command name - Test ID: {test_uuid}"

    def test_execute_tool_functionality(self, test_uuid: str) -> None:
        """Test that execute tool underlying CLI command works correctly."""
        # Test help mode (safer than actual execution)
        result = subprocess.run(["tmux-orc", "execute", "--help"], capture_output=True, text=True, timeout=10)

        assert result.returncode == 0, f"Execute tool help failed - Test ID: {test_uuid}"
        assert "Usage:" in result.stdout, f"Execute tool help missing usage - Test ID: {test_uuid}"
        assert "execute" in result.stdout, f"Execute help missing command name - Test ID: {test_uuid}"


class TestMCPToolPerformanceValidation:
    """Test performance validation of auto-generated MCP tools."""

    def test_all_tools_performance_requirements(self, test_uuid: str) -> None:
        """Test that all auto-generated tools meet performance requirements."""
        # Test the underlying CLI commands that MCP tools execute
        commands_to_test = [
            (["tmux-orc", "list", "--json"], 2.0),  # 2s max for list
            (["tmux-orc", "status", "--json"], 2.0),  # 2s max for status
            (["tmux-orc", "reflect", "--format", "json"], 2.0),  # 2s max for reflect
        ]

        performance_results = []

        for cmd, max_time in commands_to_test:
            start_time = time.perf_counter()

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)

            execution_time = time.perf_counter() - start_time
            performance_results.append((cmd, execution_time, max_time))

            assert result.returncode == 0, f"Command {' '.join(cmd)} failed - Test ID: {test_uuid}"
            assert (
                execution_time < max_time
            ), f"Command {' '.join(cmd)} took {execution_time:.3f}s (>{max_time}s) - Test ID: {test_uuid}"

        # Log performance results for analysis
        for cmd, exec_time, max_time in performance_results:
            improvement = ((max_time - exec_time) / max_time) * 100
            print(f"Performance: {' '.join(cmd)} = {exec_time:.3f}s ({improvement:.1f}% under limit)")

    def test_mcp_server_startup_performance(self, test_uuid: str) -> None:
        """Test that MCP server starts and generates tools within reasonable time."""
        start_time = time.perf_counter()

        result = subprocess.run(
            ["timeout", "15", "python", "-m", "tmux_orchestrator.mcp_server"], capture_output=True, text=True
        )

        execution_time = time.perf_counter() - start_time

        # Should discover and generate tools in <15s
        assert execution_time < 15.0, f"MCP server startup took {execution_time:.3f}s (>15s) - Test ID: {test_uuid}"

        # Should have generated all 6 tools
        assert "Generated MCP Tools" in result.stderr, f"No tools generated - Test ID: {test_uuid}"
        assert (
            "6 MCP tools" in result.stderr or result.stderr.count("→ tmux-orc") == 6
        ), f"Not all 6 tools generated - Test ID: {test_uuid}"


class TestMCPToolIntegrationValidation:
    """Test integration validation of auto-generated MCP tools."""

    def test_tools_json_output_compatibility(self, test_uuid: str) -> None:
        """Test that all tools produce MCP-compatible JSON output."""
        # Test JSON-enabled commands
        json_commands = [
            ["tmux-orc", "list", "--json"],
            ["tmux-orc", "status", "--json"],
            ["tmux-orc", "reflect", "--format", "json"],
        ]

        for cmd in json_commands:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

            if result.returncode == 0 and result.stdout:
                try:
                    # Verify JSON is valid and MCP-compatible
                    data = json.loads(result.stdout)
                    json.dumps(data)  # Should not raise exception

                    # Verify data types are MCP-compatible
                    assert (
                        isinstance(data, (dict, list, str, int, float, bool)) or data is None
                    ), f"Command {' '.join(cmd)} returned non-MCP-compatible data - Test ID: {test_uuid}"

                except json.JSONDecodeError:
                    pytest.fail(f"Command {' '.join(cmd)} returned invalid JSON - Test ID: {test_uuid}")

    def test_error_handling_validation(self, test_uuid: str) -> None:
        """Test that MCP tools handle errors gracefully."""
        # Test various invalid commands
        error_test_commands = [
            ["tmux-orc", "list", "--invalid-flag"],
            ["tmux-orc", "status", "--bad-option"],
            ["tmux-orc", "reflect", "--invalid-format", "bad"],
        ]

        for cmd in error_test_commands:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

            # Should fail gracefully (non-zero exit but no crash)
            assert result.returncode != 0, f"Invalid command {' '.join(cmd)} should fail - Test ID: {test_uuid}"

            # Should have error output (not crash)
            assert (
                len(result.stderr) > 0 or len(result.stdout) > 0
            ), f"No error output from invalid command {' '.join(cmd)} - Test ID: {test_uuid}"

    def test_cli_reflection_consistency(self, test_uuid: str) -> None:
        """Test that CLI reflection produces consistent results."""
        # Run reflect command multiple times to check consistency
        results = []

        for i in range(3):
            result = subprocess.run(
                ["tmux-orc", "reflect", "--format", "json"], capture_output=True, text=True, timeout=10
            )

            assert result.returncode == 0, f"Reflect command failed on iteration {i + 1} - Test ID: {test_uuid}"

            try:
                data = json.loads(result.stdout)
                results.append(data)
            except json.JSONDecodeError:
                pytest.fail(f"Reflect returned invalid JSON on iteration {i + 1} - Test ID: {test_uuid}")

        # Results should be consistent
        assert len(results) == 3, f"Not all reflect iterations completed - Test ID: {test_uuid}"

        # Check that command counts are consistent
        command_counts = [len(result) for result in results]
        assert all(
            count == command_counts[0] for count in command_counts
        ), f"Reflect command counts inconsistent: {command_counts} - Test ID: {test_uuid}"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
